# GraphForge v0.4.0 - REVISED Implementation Plan (Evidence-Based)

**Plan Date:** 2026-02-15
**Baseline:** 1,252 scenarios passing (32.6% compliance)
**Target:** 3,544 scenarios passing (92.4% compliance)
**Strategy:** High-impact features → Optimizer investment → Bug fixes

---

## Executive Summary

### Baseline Reality Check ✅

**Original plan assumption:** 37 scenarios (2.8%) - **INCORRECT**
**Actual baseline:** 1,252 scenarios (32.6%) - **33x better than assumed!**

**Impact:** Most features in original plan are already implemented. New plan focuses on actual gaps identified through comprehensive TCK analysis.

### Revised Strategy - Hybrid Approach (Option C)

**Phase 1: High-Impact Features** (25 hours) → +1,942 scenarios (32.6% → 83.3%)
- WITH at query start: +828 scenarios
- Map property access: +64 scenarios
- Temporal map constructors: +1,050 scenarios

**Phase 2: Optimizer Investment** (38 hours) → Architecture foundation
- Statistics framework
- Redundant traversal elimination
- Join reordering with cost model

**Phase 3: Bug Fixes & Edge Cases** (20 hours) → +350 scenarios (83.3% → 92.4%)
- Remaining 482 TCK failures
- NULL handling edge cases
- Parser edge cases

**Total Effort:** 83 hours (vs. 139 in original plan)
**Total Impact:** +2,292 scenarios → 92.4% compliance
**Time Savings:** 56 hours (40% reduction)
**TCK Improvement:** 42.4 percentage points (vs. 17.4 in original plan)

---

## Phase 1: High-Impact Features (25 hours)

### Phase 1A: WITH Clause at Query Start (6 hours) → +828 scenarios

**Issue:** #TBD (to be created)

**Problem:** Parser only allows WITH after MATCH/CREATE, not at query start

**Failing queries:**
```cypher
WITH [1, 2, 3] AS list
RETURN list[0]

WITH 1 AS x, 2 AS y
RETURN x + y

WITH {name: 'Alice'} AS person
MATCH (n:Person {name: person.name})
RETURN n
```

**Current error:** `No terminal matches 'W' in the current parser context, at line 1 col 1`

**Root cause:** Grammar defines queries as starting with read_clause (MATCH/UNWIND) or write_clause (CREATE/MERGE)

#### Implementation

**1. Grammar Changes (2 hours)**

File: `src/graphforge/parser/cypher.lark`

```lark
// Current (line ~20):
query: read_query | write_query

read_query: match_clause where_clause? return_clause?
          | unwind_clause return_clause?

// Revised:
query: read_query | write_query | with_query

read_query: match_clause where_clause? with_clause* return_clause?
          | unwind_clause with_clause* return_clause?

with_query: with_clause (read_query | write_query | with_clause)
```

**2. Planner Updates (2 hours)**

File: `src/graphforge/planner/planner.py`

- Update `plan_query()` to handle standalone WITH
- Ensure variable scoping works correctly
- Handle WITH → WITH → RETURN chains

**3. Testing (2 hours)**

- Unit tests: 10 tests for WITH at query start
- Integration tests: 15 full queries (WITH → MATCH, WITH → CREATE, WITH → WITH → RETURN)
- TCK verification: Run `tests/tck/features/official/clauses/with/`

**TCK Impact:** +828 scenarios (32.6% → 54.2%)

---

### Phase 1B: Map Property Access (3 hours) → +64 scenarios

**Issue:** #TBD

**Problem:** Cannot access properties on map literals using dot syntax

**Failing queries:**
```cypher
RETURN {name: 'Alice', age: 30}.name AS name  // Should return 'Alice'

WITH {a: 1, b: 2} AS m
RETURN m.a, m.b

RETURN [{name: 'Alice'}, {name: 'Bob'}][0].name  // Chained access
```

**Current error:** `Cannot access property on CypherMap`

**Root cause:** `evaluate_expression()` property access only handles NodeRef/EdgeRef, not CypherMap

#### Implementation

**1. Evaluator Enhancement (1.5 hours)**

File: `src/graphforge/executor/evaluator.py` (line ~300, in property access handler)

```python
def _evaluate_property_access(expr: PropertyAccess, ctx: ExecutionContext, executor: Any) -> CypherValue:
    base = evaluate_expression(expr.base, ctx, executor)

    # Existing: Handle NodeRef/EdgeRef
    if isinstance(base, (NodeRef, EdgeRef)):
        # ... existing logic ...

    # NEW: Handle CypherMap
    elif isinstance(base, CypherMap):
        key = expr.property_name
        if key in base.value:
            return base.value[key]
        else:
            return CypherNull()  # NULL for missing keys

    # NEW: Handle NULL
    elif isinstance(base, CypherNull):
        return CypherNull()

    else:
        raise TypeError(f"Cannot access property on {type(base).__name__}")
```

**2. Testing (1.5 hours)**

- Unit tests: 15 tests (map access, nested maps, NULL handling, missing keys)
- Integration tests: 10 full queries
- TCK verification: Run relevant test categories

**TCK Impact:** +64 scenarios (54.2% → 55.9%)

---

### Phase 1C: Temporal Map Constructors (16 hours) → +1,050 scenarios

**Issue:** #TBD

**Problem:** Temporal functions only accept string syntax, not map constructors

**Failing queries:**
```cypher
RETURN datetime({year: 1984, month: 10, day: 11, hour: 12}) AS dt

RETURN date({year: 2015, month: 1, day: 1}) AS d

RETURN time({hour: 12, minute: 30, second: 45, timezone: '+01:00'}) AS t

RETURN localdatetime({year: 2016, week: 1, dayOfWeek: 3}) AS ldt

RETURN duration({years: 1, months: 2, days: 3, hours: 4}) AS dur
```

**Current error:** `DATETIME expects string, got CypherMap`

**Root cause:** Temporal constructors in evaluator only handle string parsing

#### Implementation

**1. Temporal Constructor Enhancement (10 hours)**

File: `src/graphforge/executor/evaluator.py` (temporal functions section)

**Components needed:**

```python
# Parameter Sets (openCypher spec compliant)
DATETIME_PARAMS = {
    'calendar': {'year', 'month', 'day', 'hour', 'minute', 'second', 'millisecond', 'microsecond', 'nanosecond', 'timezone'},
    'week': {'year', 'week', 'dayOfWeek', 'hour', 'minute', 'second', 'millisecond', 'microsecond', 'nanosecond', 'timezone'},
    'quarter': {'year', 'quarter', 'dayOfQuarter', 'hour', 'minute', 'second', 'millisecond', 'microsecond', 'nanosecond', 'timezone'},
    'ordinal': {'year', 'ordinalDay', 'hour', 'minute', 'second', 'millisecond', 'microsecond', 'nanosecond', 'timezone'},
}

def _datetime_from_map(map_value: CypherMap) -> CypherDateTime:
    """Construct datetime from map parameters."""
    params = map_value.value

    # Determine parameter set
    if 'month' in params:
        return _datetime_from_calendar(params)
    elif 'week' in params:
        return _datetime_from_week(params)
    elif 'quarter' in params:
        return _datetime_from_quarter(params)
    elif 'ordinalDay' in params:
        return _datetime_from_ordinal(params)
    else:
        raise ValueError("Invalid datetime parameters")

def _datetime_from_calendar(params: dict) -> CypherDateTime:
    """Calendar date: year, month, day."""
    year = params.get('year', 1970)
    month = params.get('month', 1)
    day = params.get('day', 1)
    hour = params.get('hour', 0)
    minute = params.get('minute', 0)
    second = params.get('second', 0)
    microsecond = params.get('microsecond', 0) or params.get('millisecond', 0) * 1000

    tz = params.get('timezone')
    if tz:
        # Parse timezone string ('+01:00', 'UTC', etc.)
        dt = datetime(year, month, day, hour, minute, second, microsecond, tzinfo=parse_timezone(tz))
    else:
        dt = datetime(year, month, day, hour, minute, second, microsecond)

    return CypherDateTime(dt)

def _datetime_from_week(params: dict) -> CypherDateTime:
    """Week date: year, week, dayOfWeek."""
    # ISO 8601 week date arithmetic
    # Week 1 = first week with Thursday in it
    # dayOfWeek: 1 (Monday) to 7 (Sunday)
    year = params['year']
    week = params['week']
    day_of_week = params.get('dayOfWeek', 1)

    # Calculate actual date from week number
    jan4 = datetime(year, 1, 4)  # Jan 4 is always in week 1
    week1_monday = jan4 - timedelta(days=jan4.weekday())
    target_date = week1_monday + timedelta(weeks=week-1, days=day_of_week-1)

    # Add time components
    hour = params.get('hour', 0)
    minute = params.get('minute', 0)
    second = params.get('second', 0)
    microsecond = params.get('microsecond', 0) or params.get('millisecond', 0) * 1000

    dt = datetime.combine(target_date.date(), time(hour, minute, second, microsecond))

    tz = params.get('timezone')
    if tz:
        dt = dt.replace(tzinfo=parse_timezone(tz))

    return CypherDateTime(dt)

# Similar implementations for:
# - _datetime_from_quarter()
# - _datetime_from_ordinal()
# - _date_from_map()
# - _time_from_map()
# - _localdatetime_from_map()
# - _localtime_from_map()
# - _duration_from_map()
```

**2. Function Updates (3 hours)**

Update all temporal function handlers:
- `DATETIME`: Accept string OR map
- `DATE`: Accept string OR map
- `TIME`: Accept string OR map
- `LOCALDATETIME`: Accept string OR map
- `LOCALTIME`: Accept string OR map
- `DURATION`: Accept string OR map

**3. Testing (3 hours)**

- Unit tests: 60 tests covering all parameter combinations
- Integration tests: 30 full queries
- Edge cases: Invalid parameters, missing required fields, overflow values
- TCK verification: Run `tests/tck/features/official/expressions/temporal/`

**TCK Impact:** +1,050 scenarios (55.9% → 83.3%)

---

## Phase 2: Optimizer Investment (38 hours)

**Rationale:** Architectural foundation for v0.5.0 features (aggregate pushdown, cost-based optimization)

### Phase 2A: Statistics Framework (12 hours)

**Issue:** #169 (Join reordering) depends on this

File: `src/graphforge/optimizer/statistics.py` (new file)

```python
class GraphStatistics:
    """Collect and maintain graph statistics for query optimization."""

    node_count_by_label: dict[str, int]
    total_nodes: int
    edge_count_by_type: dict[str, int]
    total_edges: int
    avg_degree_by_type: dict[str, float]
    selectivity_estimates: dict[str, float]

    def update_on_create_node(self, labels: set[str]) -> None:
        """Update statistics when node is created."""
        self.total_nodes += 1
        for label in labels:
            self.node_count_by_label[label] = self.node_count_by_label.get(label, 0) + 1

    def update_on_create_edge(self, edge_type: str) -> None:
        """Update statistics when edge is created."""
        self.total_edges += 1
        self.edge_count_by_type[edge_type] = self.edge_count_by_type.get(edge_type, 0) + 1
        self._recalculate_avg_degree(edge_type)

    def estimate_scan_cost(self, labels: set[str]) -> float:
        """Estimate rows from node scan."""
        if not labels:
            return float(self.total_nodes)
        # Intersection of label counts (conservative estimate)
        return min(self.node_count_by_label.get(label, self.total_nodes) for label in labels)

    def estimate_expand_cost(self, src_count: float, edge_type: str) -> float:
        """Estimate rows from edge expansion."""
        avg_degree = self.avg_degree_by_type.get(edge_type, 10.0)  # Default: 10
        return src_count * avg_degree
```

**Integration:**
- Hook into `Graph` class in `storage/memory.py`
- Update on CREATE/DELETE operations
- Persist with graph (SQLite backend)

**Testing:** 20 unit tests, 5 integration tests

---

### Phase 2B: Redundant Traversal Elimination (8 hours)

**Issue:** #167

File: `src/graphforge/optimizer/optimizer.py`

Add optimization pass to eliminate duplicate scans:

```python
def _redundant_traversal_pass(self, operators: list[Any]) -> list[Any]:
    """Eliminate duplicate pattern scans."""
    seen_signatures: dict[str, int] = {}  # signature -> first occurrence index
    result = []

    for i, op in enumerate(operators):
        sig = self._get_operator_signature(op)

        # Check if we've seen this exact operation before
        if sig in seen_signatures:
            # Skip duplicate, reuse previous binding
            continue

        # Check for pipeline boundaries (don't merge across these)
        if isinstance(op, (With, Union, Subquery)):
            seen_signatures.clear()  # Reset tracking

        # Check for mutations (don't merge after these)
        if isinstance(op, (Create, Set, Delete, Merge)):
            seen_signatures.clear()  # Reset tracking

        seen_signatures[sig] = i
        result.append(op)

    return result
```

**Testing:** 15 unit tests, 10 integration tests, performance benchmarks

**TCK Impact:** +10 scenarios

---

### Phase 2C: Cost Model (8 hours)

File: `src/graphforge/optimizer/cost_model.py` (new file)

```python
class CostModel:
    """Estimate operator costs for join reordering."""

    def __init__(self, statistics: GraphStatistics):
        self.stats = statistics

    def estimate_scan_cost(self, op: ScanNodes) -> float:
        """Rows produced by node scan."""
        return self.stats.estimate_scan_cost(op.labels)

    def estimate_expand_cost(self, op: ExpandEdges, input_rows: float) -> float:
        """Rows produced by edge expansion."""
        return self.stats.estimate_expand_cost(input_rows, op.edge_type)

    def estimate_plan_cost(self, operators: list[Any]) -> float:
        """Total cost of operator sequence."""
        cumulative_cost = 0.0
        cardinality = 1.0

        for op in operators:
            if isinstance(op, ScanNodes):
                cardinality = self.estimate_scan_cost(op)
            elif isinstance(op, ExpandEdges):
                cardinality = self.estimate_expand_cost(op, cardinality)
            elif isinstance(op, Filter):
                cardinality *= 0.1  # Assume 10% selectivity (can be refined)

            cumulative_cost += cardinality

        return cumulative_cost
```

**Testing:** 25 unit tests, calibration against real graphs

---

### Phase 2D: Join Reordering (10 hours)

**Issue:** #169

File: `src/graphforge/optimizer/optimizer.py`

```python
def _join_reordering_pass(self, operators: list[Any], stats: GraphStatistics) -> list[Any]:
    """Reorder MATCH patterns for optimal execution cost."""

    # 1. Extract MATCH pattern groups
    patterns = self._extract_match_patterns(operators)

    # 2. Build dependency graph
    dep_graph = self._build_dependency_graph(patterns)

    # 3. Find all valid topological orderings
    valid_orders = self._find_topological_orderings(dep_graph)

    # 4. Estimate cost for each ordering
    cost_model = CostModel(stats)
    best_order = min(valid_orders, key=lambda order: cost_model.estimate_plan_cost(order))

    # 5. Rewrite operators in optimal order
    return self._rewrite_operators(operators, patterns, best_order)
```

**Safety:**
- Respect variable dependencies
- Don't cross pipeline boundaries (WITH, Union, Subquery)
- Don't reorder OPTIONAL MATCH
- Don't reorder after mutations

**Testing:** 30 unit tests, 15 integration tests, performance benchmarks

**TCK Impact:** +10 scenarios

---

## Phase 3: Bug Fixes & Edge Cases (20 hours) → +350 scenarios

**Objective:** Close remaining gaps to reach 92.4% compliance

### Systematic TCK Analysis (4 hours)

1. Run updated TCK suite after Phase 1 & 2
2. Categorize remaining ~482 failures
3. Prioritize by scenarios-per-fix ratio
4. Create GitHub issues for each category

### High-Impact Bug Categories (16 hours)

**Based on TCK baseline analysis:**

**1. Aggregation Edge Cases (5 hours) → ~120 scenarios**
- DISTINCT handling in aggregations
- NULL filtering in sum/avg/min/max
- Empty group handling

**2. NULL Handling Edge Cases (4 hours) → ~80 scenarios**
- Three-valued logic in complex expressions
- IN operator with NULL elements
- NULL in list/map operations

**3. Parser Edge Cases (4 hours) → ~80 scenarios**
- Complex nested expressions
- Edge cases in pattern syntax
- Operator precedence issues

**4. Pattern Matching Edge Cases (3 hours) → ~70 scenarios**
- Self-loops: `MATCH (n)-[r]->(n)`
- Variable-length paths with boundaries
- Relationship uniqueness

**Deliverables:** Multiple small PRs, comprehensive regression tests

---

## Phase 4: Documentation & Release (8 hours)

### Documentation Updates (5 hours)

1. **`docs/reference/opencypher-compatibility.md`**
   - Update compliance: 32.6% → 92.4%
   - Document new features (WITH, map access, temporal maps)
   - Update feature matrix

2. **`README.md`**
   - Highlight v0.4.0 capabilities
   - Add examples of new features

3. **`CHANGELOG.md`**
   - Detailed feature additions
   - Breaking changes (if any)
   - Migration guide

4. **Tutorial notebook:** `examples/notebooks/v0.4_features.ipynb`

### Performance Benchmarks (3 hours)

1. Run performance suite against v0.3.0 baseline
2. Verify optimizer improvements
3. Document any regressions (accept <5% for correctness)
4. Profile hot paths

---

## Timeline & Milestones

### Week 1-2: Phase 1A+1B (9 hours)
- WITH at query start (+828 scenarios → 54.2%)
- Map property access (+64 scenarios → 55.9%)
- **Milestone:** 2,144 scenarios passing

### Week 3-4: Phase 1C (16 hours)
- Temporal map constructors (+1,050 scenarios → 83.3%)
- **Milestone:** 3,194 scenarios passing

### Week 5-7: Phase 2A+2B+2C (28 hours)
- Statistics framework
- Redundant traversal elimination
- Cost model
- **Milestone:** Architecture foundation complete

### Week 8-9: Phase 2D (10 hours)
- Join reordering (+10 scenarios → 83.6%)
- **Milestone:** Optimizer complete, 3,204 scenarios passing

### Week 10-11: Phase 3 (20 hours)
- Bug fixes & edge cases (+350 scenarios → 92.4%)
- **Milestone:** 3,544 scenarios passing (92.4%)

### Week 12: Phase 4 (8 hours)
- Documentation & benchmarks
- **Milestone:** v0.4.0 release ready

**Total:** 12 weeks, 83 hours (~7 hours/week)

---

## Success Criteria

### MUST HAVE (Release Blockers)
- ✅ WITH at query start working
- ✅ Map property access working
- ✅ Temporal map constructors working
- ✅ Statistics framework implemented
- ✅ Redundant traversal elimination functional
- ✅ Join reordering operational
- ✅ All existing tests pass (no regressions)
- ✅ 90%+ patch coverage on new code
- ✅ 85%+ total coverage maintained

### SHOULD HAVE (Target Goals)
- ✅ 92% TCK compliance (3,544 scenarios)
- ✅ Comprehensive documentation
- ✅ Performance maintained (<5% regression)

### NICE TO HAVE (Stretch Goals)
- ✅ 95% TCK compliance if bug fixes exceed expectations
- ✅ Performance improvements from optimizer (5-20% on complex queries)
- ✅ Tutorial notebooks

---

## Risk Assessment

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Temporal arithmetic complexity | High | High | Phase approach, extensive tests, accept partial implementation |
| WITH parsing breaks existing queries | Low | Critical | Regression tests, grammar validation |
| Statistics overhead | Low | Medium | Performance tests, lazy initialization |
| Bug fix scope creep | Medium | Medium | Time-box to 20 hours, defer if needed |

---

## Comparison: Original vs. Revised Plan

| Metric | Original Plan | Revised Plan | Improvement |
|--------|---------------|--------------|-------------|
| **Effort** | 139 hours | 83 hours | **-56 hours (40%)** |
| **TCK Impact** | +661 scenarios | +2,292 scenarios | **+1,631 (247%)** |
| **Final Compliance** | 50.0% | 92.4% | **+42.4 pp** |
| **Time to 50%** | 16 weeks | 2 weeks | **87.5% faster** |
| **Wasted Effort** | 64h on done features | 0h | **Fully efficient** |
| **Optimizer** | Included | Included | Same |

**Key Advantage:** Evidence-based approach eliminates wasted effort on already-implemented features, focuses on actual gaps identified through comprehensive TCK analysis.

---

## Next Steps

1. ✅ **Phase 0 complete:** TCK baseline established, gaps identified
2. **Create GitHub issues** for Phase 1A, 1B, 1C
3. **Start Phase 1A:** WITH at query start (highest impact)

**Ready to proceed with implementation!**
